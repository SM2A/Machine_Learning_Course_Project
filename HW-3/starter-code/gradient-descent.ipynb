{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Run this code to import necessary modules. Note that the functions ``cost_function`` and ``gradient`` imported from module ``gd`` are stubs. You will need to fill in the code in ``gd.py``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from gd import cost_function, gradient, explicit_answer  # stubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simple data set\n",
    "\n",
    "Run this cell to generate and plot some data from the linear model $y \\approx -1 + 2x$, that is, $\\theta_0 = -1$ and  $\\theta_1 = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed so the program will always generate the same data\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate n random x values between -5 and 5\n",
    "n = 20\n",
    "x = 10 * np.random.rand(n) - 5\n",
    "\n",
    "# Generate y values from the model y ~= 2x - 1\n",
    "epsilon = np.random.randn(n)\n",
    "y = -1 + 2*x + epsilon\n",
    "\n",
    "plt.plot(x, y, marker='o', linestyle='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: implement the cost function\n",
    "The squared error cost function is\n",
    "$$\\frac{1}{2} \\sum_{i=1}^m \\big(h_\\theta(x^{(i)}) - y^{(i)}\\big)^2.$$\n",
    "$$h_\\theta(x^{(i)})=\\theta_0 + \\theta_1 . x^{(i)}$$\n",
    "Open the file ``gd.py`` and implement ``cost_function``. Then run this cell to test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cost_function(x, y, 0,  1))   # should print 104.772951994\n",
    "print(cost_function(x, y, 2, -1))   # should print 744.953822077\n",
    "print(cost_function(x, y, -1, 2))   # should print 14.090816198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting setup\n",
    "Run this cell. It sets up a routine ``plot_model`` that will be called later to illustrate the progress of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dense grid of (theta_0, theta_1) values\n",
    "theta0_vals = np.linspace(-10, 10)\n",
    "theta1_vals = np.linspace(-10, 10)\n",
    "[THETA0, THETA1] = np.meshgrid(theta0_vals, theta1_vals)\n",
    "\n",
    "# Define a cost function that has x and y \"baked in\"\n",
    "def mycost(theta0, theta1):\n",
    "    return cost_function(x, y, theta0, theta1)\n",
    "\n",
    "# Now vectorize this cost function and apply it simultaneously to all\n",
    "# pairs in dense grid of (theta_0, theta_1) values\n",
    "mycost_vectorized = np.vectorize(mycost)\n",
    "J_SURF = mycost_vectorized(THETA0, THETA1)\n",
    "\n",
    "# Define the test inputs\n",
    "x_test = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig = plt.figure(1, figsize=(10,4))\n",
    "\n",
    "# Create the figure\n",
    "def init_plot():\n",
    "    fig.clf()\n",
    "\n",
    "    # Build left subplot (cost function)\n",
    "    ax1 = fig.add_subplot(1, 2, 1) \n",
    "    ax1.contour(THETA0, THETA1, J_SURF, 20)\n",
    "    ax1.set_xlabel('Intercept theta_0')\n",
    "    ax1.set_ylabel('Slope theta_1')\n",
    "    ax1.set_xlim([-10, 10])\n",
    "    ax1.set_ylim([-10, 10])\n",
    "\n",
    "    # The data will be added later for these plot elements:\n",
    "    line, = ax1.plot([], [])\n",
    "    dot,  = ax1.plot([], [], marker='o')\n",
    "\n",
    "    # Build right subplot (data + current hypothesis)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.plot(x, y, marker='o', linestyle='none') \n",
    "    ax2.set_xlim([-6, 6])\n",
    "    ax2.set_ylim([-10, 10])\n",
    "\n",
    "    # The data will be added later for this:\n",
    "    hyp, = ax2.plot( x_test, 0*x_test )\n",
    "    \n",
    "    return line, dot, hyp\n",
    "\n",
    "\n",
    "# Define a function to update the plot\n",
    "def update_plot(theta_0, theta_1, line, dot, hyp):\n",
    "    line.set_xdata( np.append(line.get_xdata(), theta_0 ) )\n",
    "    line.set_ydata( np.append(line.get_ydata(), theta_1 ) )\n",
    "    dot.set_xdata([theta_0])\n",
    "    dot.set_ydata([theta_1])\n",
    "    hyp.set_ydata( theta_0 + theta_1 * x_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: implement the gradient\n",
    "1. Review the mathematical expressions for $\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1)$ and $\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1)$ for our model and cost funtion.\n",
    "1. Implement the function ``gradient`` in ``gd.py`` to return these two partial derivatives.\n",
    "\n",
    "Then run this cell to test your code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gradient(np.array([1, 2]), np.array([0, 1]), 0,  1)) # should print (2, 3)\n",
    "print(gradient(x, y, 0,  1))   # should print (3.569176215534113, -163.58239266243288)\n",
    "print(gradient(x, y, 2, -1))   # should print (10.94724640649514, -462.08205479805036)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO: implement gradient descent\n",
    "\n",
    "1. In this cell you will implement gradient descent: \n",
    "    * Select a step size\n",
    "    * Run for a fixed number of iterations (say, 20 or 200)\n",
    "    * Update `theta_0` and `theta_1` using the partial derivatives (**Hint**: use the function ``gradient`` in ``gd.py``) \n",
    "    * Record the value of the cost function attained in each iteration of gradient descent so you can examine its progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line, dot, hyp = init_plot()\n",
    "\n",
    "iters = 10  # change as needed\n",
    "\n",
    "####################################################################################\n",
    "# TODO: intialize theta_0, theta_1, step size, and an array to save the cost function\n",
    "###################################################################################\n",
    "\n",
    "for i in range(0, iters):\n",
    "    \n",
    "    #KEEP THIS CODE: this code will display progress of the \n",
    "    #algorithm as it runs \n",
    "    clear_output(wait=True)\n",
    "    update_plot(theta_0, theta_1, line, dot, hyp)\n",
    "    display(fig)\n",
    "    \n",
    "    #################################################################################\n",
    "    # TODO: \n",
    "    #    - write code to get partial derivatives (hint: call gradient in gd.py) \n",
    "    #    - update theta_0 and theta_1 with the partial derivatives and step size \n",
    "    #    - save cost function at each step (hint: call a function we gave you above)\n",
    "    #################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: compare gradient descent thetas with explicit thetas\n",
    "complete ``explicit_answer`` function in ``gd.py`` and print results of both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# TODO: Write code print both results\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: assess convergence\n",
    "Plot the iteration (x-axis) vs. cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# TODO: Write code to plot and display here\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Did the algorithm converge? (Converging means it found the actual setting of $\\theta$ that minimizes the cost. If the cost went up or did not go down as far as it could, it did not converge.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *Write your answer here*. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: experiment with step size\n",
    "\n",
    "After you have completed the implementation, do some experiments with different numbers of iterations and step sizes to assess convergence of the algorithm. Report the following:\n",
    "* A step size for which the algorithm converges to the minimum in at most 200 iterations\n",
    "* A step size for which the algorithm converges, but it takes more than 200 iterations\n",
    "* A step size for which the algorithm does not converge, no matter how many iterations are run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# TODO: Put experimental code here \n",
    "###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *Write your answer here*. **"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
